import torch
import torch.nn as nn
import math
import numpy as np
import utils
from torch.autograd import Function

class L1_Charbonnier_mean_loss_for_mosaic(nn.Module):
    """L1 Charbonnierloss."""

    def __init__(self, MSFA, device):
        super(L1_Charbonnier_mean_loss_for_mosaic, self).__init__()
        self.eps = 1e-6
        self.msfa_kernel = torch.zeros(MSFA.shape[0] * MSFA.shape[1], 1, MSFA.shape[0], MSFA.shape[1]).to(device)
        for i in range(MSFA.shape[0]):
            for j in range(MSFA.shape[1]):
                self.msfa_kernel[int(MSFA[i, j]), 0, i, j] += 1

    def forward(self, X, Y):
        assert X.shape[1] == self.msfa_kernel.shape[0] * self.msfa_kernel.shape[1]
        X_Mosaic = torch.nn.functional.conv2d(X, self.msfa_kernel, bias=None, stride=self.msfa_kernel.shape[2], groups=X.shape[1])
        X_Mosaic = torch.nn.functional.pixel_shuffle(X_Mosaic, upscale_factor=self.msfa_kernel.shape[2])
        diff = torch.add(X_Mosaic, -Y)
        error = torch.sqrt(diff * diff + self.eps)
        loss = torch.mean(error)
        return loss

class L1_Charbonnier_mean_loss(nn.Module):
    """L1 Charbonnierloss."""

    def __init__(self):
        super(L1_Charbonnier_mean_loss, self).__init__()
        self.eps = 1e-6

    def forward(self, X, Y):
        diff = torch.add(X, -Y)
        error = torch.sqrt(diff * diff + self.eps)
        loss = torch.mean(error)
        return loss

class reconstruction_loss(nn.Module):
    """reconstruction loss of raw_msfa"""

    def __init__(self, msfa_size):
        super(reconstruction_loss, self).__init__()
        self.wt = 1
        self.msfa_size = msfa_size
        # self.mse_loss = nn.MSELoss(reduce=True, size_average=False)
        self.mse_loss = L1_Charbonnier_mean_loss()

    def get_msfa(self, img_tensor, msfa_size):
        mask = torch.zeros_like(img_tensor)
        for i in range(0, msfa_size):
            for j in range(0, msfa_size):
                mask[:, i * msfa_size + j, i::msfa_size, j::msfa_size] = 1
        # buff_raw1 = mask[0, 1, :, :].cpu().detach().numpy()
        # buff_raw2 = img_tensor[0, 1, :, :].cpu().detach().numpy()
        return torch.sum(mask.mul(img_tensor), 1)

    def forward(self, X, Y):
        # loss = self.mse_loss(self.get_msfa(X, self.msfa_size), self.get_msfa(Y, self.msfa_size))
        loss = self.mse_loss(X, Y)
        return loss

class Pos2Weight(nn.Module):
    def __init__(self, outC=16, kernel_size=5, inC=1):
        super(Pos2Weight, self).__init__()
        self.inC = inC
        self.kernel_size = kernel_size
        self.outC = outC
        self.meta_block = nn.Sequential(
            nn.Linear(2, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, self.kernel_size * self.kernel_size * self.inC * self.outC)
        )

    def forward(self, x):
        output = self.meta_block(x)
        return output

class ChannelPool(nn.Module):
    def forward(self, x):
        # return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )
        # return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )
        # return torch.mean(x,1)
        return torch.mean(x, 1).unsqueeze(1)

class Shuffle_d(nn.Module):
    def __init__(self, scale=2):
        super(Shuffle_d, self).__init__()
        self.scale = scale

    def forward(self, x):
        def _space_to_channel(x, scale):
            b, C, h, w = x.size()
            Cout = C * scale ** 2
            hout = h // scale
            wout = w // scale
            x = x.contiguous().view(b, C, hout, scale, wout, scale)
            x = x.contiguous().permute(0, 1, 3, 5, 2, 4)
            x = x.contiguous().view(b, Cout, hout, wout)
            return x
        return _space_to_channel(x, self.scale)
    
class CA_AA_par_Layer1(nn.Module):
    def __init__(self, msfa_size, channel, reduction=16):
        super(CA_AA_par_Layer1, self).__init__()
        self.compress = ChannelPool()
        self.shuffledown = Shuffle_d(msfa_size)
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(msfa_size**2, msfa_size**2 // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(msfa_size**2 // reduction, msfa_size**2, bias=False),
            nn.Sigmoid()
        )
        self.shuffleup = nn.PixelShuffle(msfa_size)

        self.avg_pool1 = nn.AdaptiveAvgPool2d(1)
        self.fc1 = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        buff_x = x
        N, C, H, W = x.size()
        x = x.view(N * C, 1, H, W)  # N, C, H, W to N*C, 1, H, W
        sq_x = self.shuffledown(x)  # N*C, 1, H, W to N*C, 16, H/4, W/4
        b, c, _, _ = sq_x.size()
        y = self.avg_pool(sq_x).view(b, c)  # N*C, 16, H/4, W/4 to N*C, 16, 1, 1 to N*C, 16
        y = self.fc(y).view(b, c, 1, 1)  # N*C, 16, 1, 1
        y = y.expand_as(sq_x)  # N*C, 16, 1, 1 to N*C, 16, H/4, W/4
        ex_y = self.shuffleup(y)  # N*C, 16, H/4, W/4 to N*C, 1, H, W
        out = x * ex_y
        out = out.view(N, C, H, W)

        b, c, _, _ = buff_x.size()
        y = self.avg_pool1(buff_x).view(b, c)
        y = self.fc1(y).view(b, c, 1, 1)
        out = out * y.expand_as(out)
        return out

class _Conv_LSA_Block_msfasize(nn.Module):
    def __init__(self, msfa_size):
        super(_Conv_LSA_Block_msfasize, self).__init__()
        self.ma = CA_AA_par_Layer1(msfa_size, 64, 4)
        self.relu = nn.LeakyReLU(0.2, inplace=True)
        self.cov_block = nn.Sequential(
            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=True),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=True),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=True),
        )

    def forward(self, x):
        residual = x
        output = self.cov_block(x)
        output = self.ma(output)
        output += residual
        output = self.relu(output)
        return output

def get_WB_filter_msfa(msfa_size):
    """make a 2D weight bilinear kernel suitable for WB_Conv"""
    size = 2*msfa_size-1
    ligne = []
    colonne = []
    for i in range(size):
        if (i + 1) <= np.floor(math.sqrt(msfa_size**2)):
            ligne.append(i + 1)
            colonne.append(i + 1)
        else:
            ligne.append(ligne[i - 1] - 1.0)
            colonne.append(colonne[i - 1] - 1.0)
    BilinearFilter = np.zeros(size * size)
    for i in range(size):
        for j in range(size):
            BilinearFilter[(j + i * size)] = (ligne[i] * colonne[j] / (msfa_size**2))
    filter0 = np.reshape(BilinearFilter, (size, size))
    return torch.from_numpy(filter0).float()

class Mpattern_opt(nn.Module):
    def __init__(self, args):
        super(Mpattern_opt, self).__init__()
        self.scale = 1
        msfa_size = args.msfa_size
        self.msfa_size = args.msfa_size
        self.outC = args.msfa_size**2
        if msfa_size == 5:
            self.mcm_ksize = msfa_size+2
        elif msfa_size == 4:
            self.mcm_ksize = msfa_size + 1
        self.WB_Conv = nn.Conv2d(in_channels=msfa_size**2, out_channels=msfa_size**2, kernel_size=2*msfa_size-1, stride=1, padding=msfa_size-1, bias=False, groups=msfa_size**2)
        self.P2W = Pos2Weight(outC=self.outC, kernel_size=self.mcm_ksize)
        self.att = CA_AA_par_Layer1(msfa_size, msfa_size ** 2, 4)
        self.relu1 = nn.LeakyReLU(0.2, inplace=True)
        self.conv_input = nn.Conv2d(in_channels=msfa_size**2, out_channels=64, kernel_size=3, stride=1, padding=1,
                                          bias=True)
        self.convt_F1 = self.make_ma_layer(_Conv_LSA_Block_msfasize, msfa_size)
        self.convt_F2 = self.make_ma_layer(_Conv_LSA_Block_msfasize, msfa_size)

        self.conv_tail = nn.Conv2d(in_channels=64, out_channels=msfa_size ** 2, kernel_size=3, stride=1, padding=1, bias=True)
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
                if m.groups == msfa_size ** 2:
                    c1, c2, h, w = m.weight.data.size()
                    WB = get_WB_filter_msfa(msfa_size)
                    for i in m.parameters():
                        i.requires_grad = False
                    m.weight.data = WB.view(1, 1, h, w).repeat(c1, c2, 1, 1)
                if m.bias is not None:
                    m.bias.data.zero_()

    def make_layer(self, block):
        layers = []
        layers.append(block())
        return nn.Sequential(*layers)

    def make_ma_layer(self, block, msfa_size):
        layers = []
        layers.append(block(msfa_size))
        return nn.Sequential(*layers)

    def forward_once(self, x):
        x = self.conv_input(x)
        out = self.convt_F1(x)
        out = self.convt_F2(out)
        return out

    def repeat_y(self, y):
        scale_int = math.ceil(self.scale)
        N, C, H, W = y.size()
        y = y.view(N, C, H, 1, W, 1)

        y = torch.cat([y] * scale_int, 3)
        y = torch.cat([y] * scale_int, 5).permute(0, 3, 5, 1, 2, 4)

        return y.contiguous().view(-1, C, H, W)

    def forward(self, data, pos_mat):
        x, y = data
        WB_norelu = self.WB_Conv(x)
        N, C, H, W = y.size()
        # print(H, W)
        pos_mat = pos_mat.view(1, H, W, 2)
        pos_mat = pos_mat[:, 0:self.msfa_size, 0:self.msfa_size, :]
        pos_mat = pos_mat.contiguous().view(1, self.msfa_size ** 2, 2)
        local_weight = self.P2W(pos_mat.view(pos_mat.size(1), -1))
        local_weight = local_weight.view(self.msfa_size, self.msfa_size, self.outC * self.mcm_ksize * self.mcm_ksize)
        local_weight1 = local_weight.clone()
        cols = nn.functional.unfold(y, self.mcm_ksize, padding=(self.mcm_ksize - 1) // 2)
        cols = cols.contiguous().view(cols.size(0), 1, cols.size(1), cols.size(2),
                                      1).permute(0, 1, 3, 4, 2).contiguous()

        h_pattern_n = 1
        # This h_pattern_n can divide H / msfa_size as a int
        local_weight1 = local_weight1.repeat(h_pattern_n, int(W / self.msfa_size), 1)
        # print(local_weight1.size())
        # print(h_pattern_n, self.msfa_size, W)
        local_weight1 = local_weight1.view(h_pattern_n * self.msfa_size * W, self.outC * self.mcm_ksize * self.mcm_ksize)
        local_weight1 = local_weight1.contiguous().view(1, h_pattern_n * self.msfa_size * W, -1, self.outC)
        for i in range(0, int(H / self.msfa_size / h_pattern_n)):
            cols_buff = cols[:, 0, i * self.msfa_size * h_pattern_n * W:(i + 1) * self.msfa_size * h_pattern_n * W, :, :]
            if i == 0:
                Raw_conv_buff = torch.matmul(cols_buff, local_weight1)
            else:
                Raw_conv_buff = torch.cat([Raw_conv_buff, torch.matmul(cols_buff, local_weight1)], dim=-3)

        Raw_conv_buff = torch.unsqueeze(Raw_conv_buff, 0)
        Raw_conv_buff = Raw_conv_buff.permute(0, 1, 4, 2, 3)
        Raw_conv_buff = Raw_conv_buff.contiguous().view(N, 1, 1, self.outC, H, W)
        Raw_conv_buff = Raw_conv_buff.contiguous().view(N, self.outC, H, W)

        out = self.att(Raw_conv_buff)
        out = self.relu1(out)
        out = self.forward_once(out)
        out = self.conv_tail(out)
        return torch.add(out, WB_norelu)
        # return WB_norelu


class DWT_Function(Function):
    @staticmethod
    def forward(ctx, x, w_ll, w_lh, w_hl, w_hh):
        ctx.save_for_backward(w_ll, w_lh, w_hl, w_hh)
        ctx.shape = x.shape

        dim = x.shape[1]
        x_ll = torch.nn.functional.conv2d(x, w_ll.expand(dim, -1, -1, -1), stride=2, groups=dim)
        x_lh = torch.nn.functional.conv2d(x, w_lh.expand(dim, -1, -1, -1), stride=2, groups=dim)
        x_hl = torch.nn.functional.conv2d(x, w_hl.expand(dim, -1, -1, -1), stride=2, groups=dim)
        x_hh = torch.nn.functional.conv2d(x, w_hh.expand(dim, -1, -1, -1), stride=2, groups=dim)
        x = torch.cat([x_ll, x_lh, x_hl, x_hh], dim=1)
        return x

    @staticmethod
    def backward(ctx, dx):
        if ctx.needs_input_grad[0]:
            w_ll, w_lh, w_hl, w_hh = ctx.saved_tensors
            _, C, _, _ = ctx.shape
            dx_ll, dx_lh, dx_hl, dx_hh = dx[:, :C], dx[:, C:C * 2], dx[:, C * 2:C * 3], dx[:, C * 3:]

            dx_x_ll = torch.nn.functional.conv_transpose2d(dx_ll, w_ll.expand(C, -1, -1, -1) * 4, stride=2, groups=C)
            dx_x_lh = torch.nn.functional.conv_transpose2d(dx_lh, w_lh.expand(C, -1, -1, -1) * 4, stride=2, groups=C)
            dx_x_hl = torch.nn.functional.conv_transpose2d(dx_hl, w_hl.expand(C, -1, -1, -1) * 4, stride=2, groups=C)
            dx_x_hh = torch.nn.functional.conv_transpose2d(dx_hh, w_hh.expand(C, -1, -1, -1) * 4, stride=2, groups=C)
            return dx_x_ll + dx_x_lh + dx_x_hl + dx_x_hh, None, None, None, None
        else:
            return dx, None, None, None, None


class DWT_2D(nn.Module):
    def __init__(self):
        super(DWT_2D, self).__init__()
        w_ll = torch.tensor([[[[0.25, 0.25], [0.25, 0.25]]]], dtype=torch.float32, requires_grad=False)
        w_lh = torch.tensor([[[[0.25, 0.25], [-0.25, -0.25]]]], dtype=torch.float32, requires_grad=False)
        w_hl = torch.tensor([[[[0.25, -0.25], [0.25, -0.25]]]], dtype=torch.float32, requires_grad=False)
        w_hh = torch.tensor([[[[0.25, -0.25], [-0.25, 0.25]]]], dtype=torch.float32, requires_grad=False)

        self.register_buffer('w_ll', w_ll)
        self.register_buffer('w_lh', w_lh)
        self.register_buffer('w_hl', w_hl)
        self.register_buffer('w_hh', w_hh)

        self.w_ll = w_ll.to(dtype=torch.float32)
        self.w_lh = w_lh.to(dtype=torch.float32)
        self.w_hl = w_hl.to(dtype=torch.float32)
        self.w_hh = w_hh.to(dtype=torch.float32)

    def forward(self, x):
        return DWT_Function.apply(x, self.w_ll, self.w_lh, self.w_hl, self.w_hh)


class IDWT_Function(Function):
    @staticmethod
    def forward(ctx, x, filters):
        ctx.save_for_backward(filters)
        ctx.shape = x.shape

        _, C, _, _ = x.shape
        w_ll, w_lh, w_hl, w_hh = torch.unbind(filters, dim=0)
        x_ll, x_lh, x_hl, x_hh = x[:, :C // 4], x[:, C // 4:C * 2 // 4], x[:, C * 2 // 4:C * 3 // 4], x[:, C * 3 // 4:]
        x_1_ll = torch.nn.functional.conv_transpose2d(x_ll, w_ll.expand(C // 4, -1, -1, -1), stride=2, groups=C // 4)
        x_1_lh = torch.nn.functional.conv_transpose2d(x_lh, w_lh.expand(C // 4, -1, -1, -1), stride=2, groups=C // 4)
        x_1_hl = torch.nn.functional.conv_transpose2d(x_hl, w_hl.expand(C // 4, -1, -1, -1), stride=2, groups=C // 4)
        x_1_hh = torch.nn.functional.conv_transpose2d(x_hh, w_hh.expand(C // 4, -1, -1, -1), stride=2, groups=C // 4)
        return x_1_ll + x_1_lh + x_1_hl + x_1_hh

    @staticmethod
    def backward(ctx, dx):
        if ctx.needs_input_grad[0]:
            filters = ctx.saved_tensors
            filters = filters[0]
            _, C, _, _ = ctx.shape
            C //= 4

            w_ll, w_lh, w_hl, w_hh = torch.unbind(filters, dim=0)
            x_ll = torch.nn.functional.conv2d(dx, w_ll.unsqueeze(1).expand(C, -1, -1, -1) / 4, stride=2, groups=C)
            x_lh = torch.nn.functional.conv2d(dx, w_lh.unsqueeze(1).expand(C, -1, -1, -1) / 4, stride=2, groups=C)
            x_hl = torch.nn.functional.conv2d(dx, w_hl.unsqueeze(1).expand(C, -1, -1, -1) / 4, stride=2, groups=C)
            x_hh = torch.nn.functional.conv2d(dx, w_hh.unsqueeze(1).expand(C, -1, -1, -1) / 4, stride=2, groups=C)
            dx = torch.cat([x_ll, x_lh, x_hl, x_hh], dim=1)
        return dx, None


class IDWT_2D(nn.Module):
    def __init__(self):
        super(IDWT_2D, self).__init__()
        w_ll = torch.tensor([[[[1, 1], [1, 1]]]], dtype=torch.float32, requires_grad=False)
        w_lh = torch.tensor([[[[1, 1], [-1, -1]]]], dtype=torch.float32, requires_grad=False)
        w_hl = torch.tensor([[[[1, -1], [1, -1]]]], dtype=torch.float32, requires_grad=False)
        w_hh = torch.tensor([[[[1, -1], [-1, 1]]]], dtype=torch.float32, requires_grad=False)

        filters = torch.cat([w_ll, w_lh, w_hl, w_hh], dim=0)
        self.register_buffer('filters', filters)
        self.filters = filters

    def forward(self, x):
        return IDWT_Function.apply(x, self.filters)


class raise_channel(nn.Module):
    def __init__(self, in_channel, target_channel):
        super(raise_channel, self).__init__()
        self.raise_conv = nn.Sequential(
            nn.Conv2d(in_channel, target_channel, 5, 1, 2, bias=True),
            nn.PReLU(num_parameters=target_channel, init=0.01),
            nn.Conv2d(target_channel, target_channel, 3, 1, 1, bias=True),
        )

    def forward(self, x):
        x = self.raise_conv(x)
        return x


class reduce_channel(nn.Module):
    def __init__(self, ms_target_channel, L_up_channel):
        super(reduce_channel, self).__init__()
        self.reduce_conv = nn.Sequential(
            nn.Conv2d(ms_target_channel, ms_target_channel, 3, 1, 1, bias=True),
            nn.PReLU(num_parameters=ms_target_channel, init=0.01),
            nn.Conv2d(ms_target_channel, L_up_channel, 3, 1, 1, bias=True),
            nn.Conv2d(L_up_channel, L_up_channel, 3, 1, 1, bias=True),
        )

    def forward(self, x):
        return self.reduce_conv(x)


class FFN(nn.Module):
    def __init__(self, in_channel, FFN_channel, out_channel):
        super(FFN, self).__init__()
        self.FFN_channel, self.out_channel = FFN_channel, out_channel
        self.linear_1 = nn.Linear(in_channel, FFN_channel)
        self.conv1 = nn.Conv2d(FFN_channel, FFN_channel, 3, 1, 1, bias=True)
        self.conv2 = nn.Conv2d(FFN_channel, FFN_channel, 1, 1, 0, bias=True)
        self.linear_2 = nn.Linear(FFN_channel, out_channel)
        self.act = nn.PReLU(num_parameters=FFN_channel, init=0.01)

    def forward(self, x):
        B, C, H, W = x.shape
        rs1 = self.linear_1(x.permute(0, 2, 3, 1).reshape(B, -1, C)).permute(0, 2, 1).reshape(B, self.FFN_channel, H, W)
        rs2 = self.act(self.conv1(rs1))
        rs3 = self.conv2(rs2) + rs1
        rs4 = self.linear_2(rs3.permute(0, 2, 3, 1).reshape(B, -1, self.FFN_channel)).permute(0, 2, 1).reshape(B, self.out_channel, H, W)
        return rs4


class FFN_2(nn.Module):
    def __init__(self, in_channel, FFN_channel, out_channel):
        super(FFN_2, self).__init__()
        self.conv1 = nn.Conv2d(in_channel, FFN_channel, 3, 1, 2, bias=True, dilation=2)
        self.conv2 = nn.Conv2d(FFN_channel, FFN_channel, 3, 1, 1, bias=True)
        self.conv3 = nn.Conv2d(FFN_channel, FFN_channel, 1, 1, 0, bias=True)
        self.conv4 = nn.Conv2d(FFN_channel, out_channel, 3, 1, 1, bias=True)
        self.act = nn.PReLU(num_parameters=FFN_channel, init=0.01)

    def forward(self, x):
        rs1 = self.conv1(x)
        rs2 = self.act(self.conv2(rs1))
        rs3 = self.conv3(rs2) + rs1
        rs4 = self.conv4(rs3)
        return rs4


class conv_IDWT(nn.Module):
    def __init__(self, channel):
        super(conv_IDWT, self).__init__()
        self.res_block = resblock(channel=channel)
        self.IDWT = IDWT_2D()

    def forward(self, x):
        rs1 = self.IDWT(x)
        rs2 = self.res_block(rs1)
        return rs2


class resblock(nn.Module):
    def __init__(self, channel):
        super(resblock, self).__init__()
        self.conv1 = nn.Conv2d(channel, channel, 3, 1, 1, bias=True)
        self.conv2 = nn.Conv2d(channel, channel, 3, 1, 1, bias=True)
        self.act = nn.PReLU(num_parameters=channel, init=0.01)

    def forward(self, x):
        rs1 = self.act(self.conv1(x))
        rs2 = self.conv2(rs1) + x
        return rs2


class DWC(nn.Module):
    def __init__(self, channel):
        super(DWC, self).__init__()
        self.linear = nn.Linear(channel, channel, bias=True)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        B, C, H, W = x.shape
        rs1 = self.linear(x.permute(0, 2, 3, 1).reshape(B, -1, C))
        rs2 = self.sigmoid(rs1).permute(0, 2, 1).reshape(B, C, H, W)
        return rs2


class Attention(nn.Module):
    def __init__(self, channel, head_channel, dropout):
        super(Attention, self).__init__()
        self.head_channel, self.channel = head_channel, channel
        self.q = nn.Sequential(
            nn.LayerNorm(channel),
            nn.Linear(channel, channel * 2, bias=True),
            nn.LeakyReLU(0.01),
            nn.Linear(channel * 2, channel, bias=True),
            nn.Dropout(dropout),
        )
        self.k = nn.Sequential(
            nn.LayerNorm(channel),
            nn.Linear(channel, channel * 2, bias=True),
            nn.LeakyReLU(0.01),
            nn.Linear(channel * 2, channel, bias=True),
            nn.Dropout(dropout),
        )
        self.v = nn.Sequential(
            nn.LayerNorm(channel),
            nn.Linear(channel, channel * 2, bias=True),
            nn.LeakyReLU(0.01),
            nn.Linear(channel * 2, channel, bias=True),
            nn.Dropout(dropout),
        )
        self.scale = head_channel ** 0.5
        self.num_head = channel // self.head_channel
        self.mlp_1 = nn.Sequential(
            nn.Linear(channel, channel * 2, bias=True),
            nn.LeakyReLU(0.01),
            nn.Linear(channel * 2, channel, bias=True),
            nn.Dropout(dropout),
        )
        self.mlp_2 = nn.Sequential(
            nn.Linear(channel, channel * 2, bias=True),
            nn.LeakyReLU(0.01),
            nn.Linear(channel * 2, channel, bias=True),
            nn.Dropout(dropout),
        )

    def forward(self, q, k, v):
        B, q_C, H, W = q.shape
        _, v_C, _, _ = v.shape
        q_attn = self.q(q.permute(0, 2, 3, 1).reshape(B, -1, q_C)).reshape(B, -1, self.num_head, self.head_channel).permute(0, 2, 1, 3)
        k_attn = self.k(k.permute(0, 2, 3, 1).reshape(B, -1, q_C)).reshape(B, -1, self.num_head, self.head_channel).permute(0, 2, 3, 1)
        v_attn_1 = self.v(v.permute(0, 2, 3, 1).reshape(B, -1, v_C))
        v_attn = v_attn_1.reshape(B, -1, self.num_head, self.head_channel).permute(0, 2, 1, 3)
        attn = ((q_attn @ k_attn) / self.scale).softmax(dim=-1)
        x = (attn @ v_attn).permute(0, 2, 1, 3).reshape(B, -1, v_C)
        rs1 = v_attn_1.permute(0, 2, 1).reshape(B, q_C, H, W) + self.mlp_1(x).permute(0, 2, 1).reshape(B, v_C, H, W)
        rs2 = rs1 + self.mlp_2(rs1.permute(0, 2, 3, 1).reshape(B, -1, v_C)).permute(0, 2, 1).reshape(B, v_C, H, W)
        return rs2


class combine(nn.Module):
    def __init__(self, channel):
        super(combine, self).__init__()
        self.resblock = resblock(channel=channel)
        self.a = nn.Parameter(torch.tensor(0.33), requires_grad=True)
        self.b = nn.Parameter(torch.tensor(0.33), requires_grad=True)

    def forward(self, x1, x2, x3):
        rs1 = self.a * x1 + self.b * x2 + (1 - self.a - self.b) * x3
        rs2 = self.resblock(rs1)
        return rs2


class S_MWiT(nn.Module):
    def __init__(self, pan_ll_channel, L_up_channel, head_channel, dropout):
        super(S_MWiT, self).__init__()
        self.pan_ll_channel = pan_ll_channel
        self.WD = DWT_2D()
        self.v_ll_attn = Attention(channel=L_up_channel, head_channel=head_channel, dropout=dropout)
        self.v_lh_attn = Attention(channel=L_up_channel, head_channel=head_channel, dropout=dropout)
        self.v_hl_attn = Attention(channel=L_up_channel, head_channel=head_channel, dropout=dropout)
        self.v_hh_attn = Attention(channel=L_up_channel, head_channel=head_channel, dropout=dropout)
        self.conv_idwt_pan = conv_IDWT(channel=pan_ll_channel)
        self.wd_ll_conv = DWC(channel=pan_ll_channel)
        self.wd_lh_conv = DWC(channel=pan_ll_channel)
        self.wd_hl_conv = DWC(channel=pan_ll_channel)
        self.wd_hh_conv = DWC(channel=pan_ll_channel)
        self.conv_idwt_up = conv_IDWT(channel=L_up_channel)
        self.combine = combine(channel=L_up_channel)
        self.resblock = resblock(channel=L_up_channel)
        self.resblock_1 = resblock(channel=L_up_channel)
        self.mlp = FFN(in_channel=L_up_channel, FFN_channel=L_up_channel // 2, out_channel=L_up_channel)
        self.conv_x = FFN_2(in_channel=L_up_channel, FFN_channel=L_up_channel // 2, out_channel=L_up_channel)
        self.conv_v = FFN_2(in_channel=L_up_channel, FFN_channel=L_up_channel // 2, out_channel=L_up_channel)

    def forward(self, pan_ll, L_up, back_img):
        wd_ll, wd_lh, wd_hl, wd_hh = torch.split(self.WD(pan_ll), [self.pan_ll_channel, self.pan_ll_channel, self.pan_ll_channel, self.pan_ll_channel], dim=1)

        pre_v = self.combine(x1=wd_ll, x2=L_up, x3=self.mlp(back_img))
        v = self.resblock(pre_v)

        v_ll = self.v_ll_attn(q=wd_ll, k=wd_ll, v=v)
        v_lh = self.v_lh_attn(q=wd_lh, k=wd_ll, v=v)
        v_hl = self.v_hl_attn(q=wd_hl, k=wd_ll, v=v)
        v_hh = self.v_hh_attn(q=wd_hh, k=wd_ll, v=v)
        v_idwt = self.conv_idwt_up(torch.cat([v_ll, v_lh, v_hl, v_hh], dim=1))

        x_idwt = self.conv_idwt_pan(
            torch.cat([self.wd_ll_conv(wd_ll), self.wd_lh_conv(wd_lh), self.wd_hl_conv(wd_hl), self.wd_hh_conv(wd_hh)],
                      dim=1))
        x_1 = self.conv_x(x_idwt) + self.conv_v(v_idwt)
        x = self.resblock_1(x_1)
        return x


class F_MWiT(nn.Module):
    def __init__(self, pan_channel, L_up_channel, head_channel, dropout):
        super(F_MWiT, self).__init__()
        self.s_mwit = S_MWiT(pan_ll_channel=pan_channel, L_up_channel=L_up_channel, head_channel=head_channel, dropout=dropout)
        self.combine = combine(channel=L_up_channel)
        self.mlp = FFN(in_channel=L_up_channel, FFN_channel=L_up_channel // 2, out_channel=L_up_channel)
        self.resblock = resblock(channel=L_up_channel)

    def forward(self, pan, L_up, back_img, lms):
        x = self.s_mwit(pan_ll=pan, L_up=L_up, back_img=back_img)
        x = self.combine(x1=pan, x2=lms, x3=self.mlp(x))
        x = self.resblock(x)
        return x

# small scale
class L_MWiT(nn.Module):
    def __init__(self, pan_ll_channel, L_up_channel, head_channel, dropout):
        super(L_MWiT, self).__init__()
        self.pan_ll_channel = pan_ll_channel
        self.WD = DWT_2D()
        self.v_ll_attn = Attention(channel=L_up_channel, head_channel=head_channel, dropout=dropout)
        self.v_lh_attn = Attention(channel=L_up_channel, head_channel=head_channel, dropout=dropout)
        self.v_hl_attn = Attention(channel=L_up_channel, head_channel=head_channel, dropout=dropout)
        self.v_hh_attn = Attention(channel=L_up_channel, head_channel=head_channel, dropout=dropout)
        self.mlp = FFN(in_channel=L_up_channel, FFN_channel=L_up_channel // 2, out_channel=L_up_channel)
        self.wd_ll_conv = DWC(channel=pan_ll_channel)
        self.wd_lh_conv = DWC(channel=pan_ll_channel)
        self.wd_hl_conv = DWC(channel=pan_ll_channel)
        self.wd_hh_conv = DWC(channel=pan_ll_channel)
        self.conv_idwt_pan = conv_IDWT(channel=pan_ll_channel)
        self.conv_idwt_up = conv_IDWT(channel=L_up_channel)
        self.combine = combine(channel=L_up_channel)
        self.resblock = resblock(channel=L_up_channel)
        self.resblock_1 = resblock(channel=L_up_channel)
        self.conv_x = FFN_2(in_channel=L_up_channel, FFN_channel=L_up_channel // 2, out_channel=L_up_channel)
        self.conv_v = FFN_2(in_channel=L_up_channel, FFN_channel=L_up_channel // 2, out_channel=L_up_channel)

    def forward(self, pan_ll, back_img, L_up):
        wd_ll, wd_lh, wd_hl, wd_hh = torch.split(self.WD(pan_ll),
                                                 [self.pan_ll_channel, self.pan_ll_channel, self.pan_ll_channel,
                                                  self.pan_ll_channel], dim=1)

        pre_v = self.combine(x1=wd_ll, x2=L_up, x3=self.mlp(back_img))
        v = self.resblock(pre_v)

        v_ll = self.v_ll_attn(q=wd_ll, k=wd_ll, v=v)
        v_lh = self.v_lh_attn(q=wd_lh, k=wd_ll, v=v)
        v_hl = self.v_hl_attn(q=wd_hl, k=wd_ll, v=v)
        v_hh = self.v_hh_attn(q=wd_hh, k=wd_ll, v=v)
        v_idwt = self.conv_idwt_up(torch.cat([v_ll, v_lh, v_hl, v_hh], dim=1))

        x_idwt = self.conv_idwt_pan(
            torch.cat([self.wd_ll_conv(wd_ll), self.wd_lh_conv(wd_lh), self.wd_hl_conv(wd_hl), self.wd_hh_conv(wd_hh)],
                      dim=1))
        x_1 = self.conv_x(x_idwt) + self.conv_v(v_idwt)
        x = self.resblock_1(x_1)
        return x


class HWViT(nn.Module):
    def __init__(self, L_up_channel, pan_channel, pan_target_channel, ms_target_channel, head_channel, dropout):
        super(HWViT, self).__init__()
        self.pan_channel = pan_channel
        self.lms = nn.Sequential(
            nn.Conv2d(L_up_channel, L_up_channel * 4, 3, 1, 1, bias=True),
            nn.PixelShuffle(2),
        )
        self.pan_raise_channel = raise_channel(in_channel=pan_channel, target_channel=pan_target_channel)
        self.lms_raise_channel = raise_channel(in_channel=L_up_channel, target_channel=ms_target_channel)
        self.ms_raise_channel = raise_channel(in_channel=L_up_channel, target_channel=ms_target_channel)
        self.reduce_channel = reduce_channel(ms_target_channel=ms_target_channel, L_up_channel=L_up_channel)
        self.F_MWiT_block = F_MWiT(L_up_channel=ms_target_channel, pan_channel=pan_target_channel, head_channel=head_channel, dropout=dropout)
        self.L_MWiT_block = L_MWiT(L_up_channel=ms_target_channel, pan_ll_channel=pan_target_channel, head_channel=head_channel, dropout=dropout)
        self.lms_down_2 = nn.AvgPool2d(kernel_size=2, stride=2)
        self.lms_down_4 = nn.AvgPool2d(kernel_size=4, stride=4)
        self.pan_down_2 = nn.AvgPool2d(kernel_size=2, stride=2)
        self.act_1 = nn.PReLU(num_parameters=L_up_channel, init=0.01)
        self.act_2 = nn.PReLU(num_parameters=L_up_channel, init=0.01)

    def forward(self, ms, pan):
        lms = torch.nn.functional.interpolate(ms, scale_factor=2, mode='bilinear')
        pan = self.pan_raise_channel(pan)
        lms_1 = self.act_1(self.lms(ms) + lms)
        lms_2 = self.lms_raise_channel(lms_1)
        back_1 = self.L_MWiT_block(pan_ll=self.lms_down_2(pan), back_img=self.lms_down_2(self.ms_raise_channel(ms)), L_up=self.lms_down_4(lms_2))
        back_2 = self.F_MWiT_block(pan=pan, L_up=self.lms_down_2(lms_2), back_img=back_1, lms=lms_2)
        back = self.reduce_channel(back_2)
        result = self.act_2(back + lms_1)
        return result